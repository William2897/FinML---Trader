# Project Context: FinML-Trader

## 1. Project Goal
The primary goal of this project is to build a complete, end-to-end MLOps platform for developing and backtesting algorithmic trading strategies. We will ingest financial data, train machine learning models to generate trading signals, automate the entire workflow, and visualize the results.

## 2. Core Principles
- **No-Cost Tooling:** We must exclusively use free, open-source software that can be run locally. Do not suggest paid cloud services. Local Docker containers are the preferred method for running services like databases or MLOps tools.
- **Modular Code:** All Python code should be clean, well-documented with docstrings, use type hints, and be organized into logical modules and functions.
- **Step-by-Step Development:** We will build this project in the phases outlined below. Focus on completing the current phase before moving to the next.

## 3. Technology Stack
- **Language:** Python 3.10+
- **Data Ingestion:** `yfinance`
- **Data Processing:** `pandas`, `numpy`
- **Database:** PostgreSQL (via Docker)
- **ML Frameworks:** `scikit-learn` (for XGBoost), `pytorch` (for LSTMs), `transformers` (for FinBERT)
- **Experiment Tracking:** MLflow
- **Workflow Orchestration:** Apache Airflow
- **API & Deployment:** FastAPI, Docker, Kubernetes (via Minikube)
- **CI/CD:** GitHub Actions
- **Frontend/Dashboard:** Streamlit

---

## 4. Phased Development Plan

### Phase 1: The Data Foundation
*   **Goal:** Establish a robust, automated data pipeline.
*   **Tasks:**
    1.  Scaffold the initial Python project structure (`src` layout).
    2.  Create a `docker-compose.yml` file to run a local PostgreSQL instance.
    3.  Write a Python script (`src/data_ingestion.py`) to:
        *   Connect to the local PostgreSQL database.
        *   Create a table for daily stock prices if it doesn't exist.
        *   Fetch historical data using `yfinance` and store it in the table.

### Phase 2: Model Development & Experiment Tracking
*   **Goal:** Build and track multiple predictive models.
*   **Tasks:**
    1.  **Time-Series Model:** Develop a script to train a classic model (e.g., XGBoost) to predict the next day's price movement.
    2.  **Sentiment Model:** Develop a script to fine-tune a pre-trained `FinBERT` model from Hugging Face on financial news data to classify sentiment.
    3.  **Integrate MLflow:** Modify the training scripts to log all experiments, including parameters, metrics, and model artifacts, to a local MLflow tracking server.

### Phase 3: Automation & MLOps
*   **Goal:** Automate the entire workflow from data ingestion to model training.
*   **Tasks:**
    1.  **Orchestration with Airflow:** Set up Apache Airflow locally (via Docker) and create a DAG that automates the entire Phase 1 & 2 pipeline: `fetch_data` -> `train_models`.
    2.  **API for Inference:** Create a simple API using **FastAPI** that loads the best-performing model from MLflow and exposes an endpoint to get the latest trading signal.
    3.  **Containerize Application:** Use Docker to containerize the FastAPI application.
    4.  **CI/CD Pipeline:** Set up a basic CI pipeline using **GitHub Actions** to run tests (e.g., using `pytest`) on every push.

### Phase 4: Backtesting & Visualization
*   **Goal:** Evaluate the strategy's performance and create a user-friendly interface.
*   **Tasks:**
    1.  **Backtesting Engine:** Write a Python module that takes historical signals generated by the models and simulates a trading strategy, calculating key performance metrics (e.g., total return, Sharpe ratio).
    2.  **Visualization Dashboard:** Build a simple web dashboard using **Streamlit** that:
        *   Connects to the database and API.
        *   Displays the backtesting results with charts (e.g., an equity curve).
        *   Shows the current trading signal from the live model.

---